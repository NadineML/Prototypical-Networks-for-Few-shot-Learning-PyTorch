{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "Prototypical Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadineML/Prototypical-Networks-for-Few-shot-Learning-PyTorch/blob/master/working-old-Prototypical_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "* [Setup](#scrollTo=T0ay-ybnOqDn)\n",
        "* [Configuration](#scrollTo=WKwMFW2Mnf_a&uniqifier=37)\n",
        "  * **interactive** [mount drive](#scrollTo=g9vZGh5SgFmQ)\n",
        "  * **interactive** [decide if you want to set parameters manually or load config.json](#scrollTo=UyCwYDbpR7Jv) \n",
        "  * **interactive** [set parameters manually, if desired](#scrollTo=UyCwYDbpR7Jv) \n",
        "  \n",
        "* [Datasets](#scrollTo=yq7s0Hiy4F_s)\n",
        "* [Execution for manually set parameters](#scrollTo=y7Tf7K6OyW_i)\n",
        "* [Execution for automatically set parameters](#scrollTo=Ra1IpSVMOJuW)\n",
        "* [Sources](#scrollTo=HexvGfNtzwfV)\n",
        "\n"
      ],
      "metadata": {
        "id": "E1e70_tknZng"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ay-ybnOqDn"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and Installs"
      ],
      "metadata": {
        "id": "NOeVexONlSUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyfsl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWu2Ge_zLYwJ",
        "outputId": "dd791c32-4e0e-4e1d-acca-393ba0584c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyfsl\n",
            "  Downloading easyfsl-0.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from easyfsl) (1.1.5)\n",
            "Collecting loguru>=0.5.0\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from easyfsl) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from easyfsl) (1.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from easyfsl) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from easyfsl) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->easyfsl) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->easyfsl) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->easyfsl) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->easyfsl) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->easyfsl) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->easyfsl) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->easyfsl) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.7.0->easyfsl) (7.1.2)\n",
            "Installing collected packages: loguru, easyfsl\n",
            "Successfully installed easyfsl-0.2.0 loguru-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gD5jDtGZ7Krp"
      },
      "source": [
        "#@title import necessary modules { form-width: \"15%\", display-mode: \"form\" }\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader,Sampler, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "from torchvision.models import resnet18, alexnet, squeezenet1_0, googlenet\n",
        "from tqdm import tqdm\n",
        "\n",
        "from easyfsl.utils import plot_images, sliding_average\n",
        "\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "import csv\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Classes"
      ],
      "metadata": {
        "id": "FnPSIaSmlg2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title StatsTracker Class  { form-width: \"15%\", display-mode: \"form\" }\n",
        "class StatsTracker():\n",
        "    def __init__(\n",
        "        self, \n",
        "        n_way: int, \n",
        "        n_shot: int, \n",
        "        n_query: int, \n",
        "        n_evaluation_tasks: int, \n",
        "        image_size: int, \n",
        "        batch_size: int, \n",
        "        n_training_episodes: int, \n",
        "        n_validation_tasks: int, \n",
        "        pretrained_net: str\n",
        "      ):\n",
        "      \n",
        "      super(StatsTracker, self).__init__()\n",
        "      self.n_way = n_way\n",
        "      self.n_shot = n_shot\n",
        "      self.n_query = n_query\n",
        "      self.n_evaluation_tasks = n_evaluation_tasks\n",
        "      self.image_size = image_size\n",
        "      self.batch_size = batch_size\n",
        "      self.n_training_episodes = n_training_episodes\n",
        "      self.n_validation_tasks = n_validation_tasks\n",
        "      self.backbone = pretrained_net\n",
        "      self.base_performance = 0\n",
        "      self._loss_list = []\n",
        "      self.epochs = -1\n",
        "      self._acc_logging = {}\n",
        "      \n",
        "      return\n",
        "\n",
        "\n",
        "    @property\n",
        "    def loss_list(self):\n",
        "        return self._loss_list\n",
        "\n",
        "    @loss_list.setter\n",
        "    def loss_list(self, value):\n",
        "        self._loss_list = value\n",
        "        self.epochs = len(self._loss_list)-1\n",
        "\n",
        "\n",
        "\n",
        "    @property\n",
        "    def acc_logging(self):\n",
        "        return self._acc_logging\n",
        "\n",
        "    @acc_logging.setter\n",
        "    def acc_logging(self, acc):\n",
        "        self._acc_logging.update({self.epochs : acc})\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-rQ0wlS76-9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iCRwLATr7Krr"
      },
      "source": [
        "#@title PrototypicalNetworks Class { form-width: \"15%\", display-mode: \"form\" }\n",
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        global tracker\n",
        "        global VERBOSE\n",
        "        self.backbone = backbone\n",
        "        #print(tracker)\n",
        "        if VERBOSE > 0:\n",
        "            print(\"Created Prototypical Network model with pretrained {} as a backbone\\n\".format(\n",
        "                tracker.backbone))\n",
        "        if VERBOSE == 2:\n",
        "            print(\"\\nNetwork architecture: \\n{}\".format(backbone))\n",
        "    def forward(\n",
        "        self,\n",
        "        support_images: torch.Tensor,\n",
        "        support_labels: torch.Tensor,\n",
        "        query_images: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "\n",
        "        # And here is the super complicated operation to transform those distances into classification scores!\n",
        "        scores = -dists\n",
        "        return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ModTaskSampler Class { form-width: \"15%\", display-mode: \"form\" }\n",
        "class ModTaskSampler(Sampler):\n",
        "    \"\"\"\n",
        "    This is a modified version of easyfsl.data_tools.TaskSampler.\n",
        "    Samples batches in the shape of few-shot classification tasks. At each iteration, it will sample\n",
        "    n_way classes, and then sample support and query images from these classes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataset: Dataset, n_way: int, n_shot: int, n_query: int, n_tasks: int\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset: dataset from which to sample classification tasks. Must have a field 'items_per_label': a\n",
        "                dict of the structure {label1 : [idx_occurence1, idx_occurence2, ...], label2 : [...]} and \n",
        "            n_way: number of classes in one task\n",
        "            n_shot: number of support images for each class in one task\n",
        "            n_query: number of query images for each class in one task\n",
        "            n_tasks: number of tasks to sample\n",
        "        \"\"\"\n",
        "        super().__init__(data_source=None)\n",
        "        self.n_way = n_way\n",
        "        self.n_shot = n_shot\n",
        "        self.n_query = n_query\n",
        "        self.n_tasks = n_tasks\n",
        "\n",
        "        self.items_per_label = {}\n",
        "        assert hasattr(\n",
        "            dataset, \"items_per_label\"\n",
        "        ), \"TaskSampler needs a dataset with a field 'items_per_label' containing the labels of all images and their occurrences.\"\n",
        "        self.items_per_label = dataset.items_per_label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.n_tasks):\n",
        "            yield torch.cat(\n",
        "                [\n",
        "                    # pylint: disable=not-callable\n",
        "                    torch.tensor(\n",
        "                        random.sample(\n",
        "                            self.items_per_label[label], self.n_shot + self.n_query\n",
        "                        )\n",
        "                    )\n",
        "                    # pylint: enable=not-callable\n",
        "                    for label in random.sample(self.items_per_label.keys(), self.n_way)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    def episodic_collate_fn(\n",
        "        self, input_data: List[Tuple[torch.Tensor, int]]\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[int]]:\n",
        "        \"\"\"\n",
        "        Collate function to be used as argument for the collate_fn parameter of episodic\n",
        "            data loaders.\n",
        "        Args:\n",
        "            input_data: each element is a tuple containing:\n",
        "                - an image as a torch Tensor\n",
        "                - the label of this image\n",
        "        Returns:\n",
        "            tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
        "                - support images,\n",
        "                - their labels,\n",
        "                - query images,\n",
        "                - their labels,\n",
        "                - the dataset class ids of the class sampled in the episode\n",
        "        \"\"\"\n",
        "\n",
        "        true_class_ids = list({x[1] for x in input_data})\n",
        "\n",
        "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data])\n",
        "        all_images = all_images.reshape(\n",
        "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
        "        )\n",
        "        # pylint: disable=not-callable\n",
        "        all_labels = torch.tensor(\n",
        "            [true_class_ids.index(x[1]) for x in input_data]\n",
        "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
        "        # pylint: enable=not-callable\n",
        "\n",
        "        support_images = all_images[:, : self.n_shot].reshape(\n",
        "            (-1, *all_images.shape[2:])\n",
        "        )\n",
        "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
        "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
        "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
        "\n",
        "        return (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            true_class_ids,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "0vJcLesNEYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CombinedDataset Class  { form-width: \"15%\", display-mode: \"form\" }\n",
        "\n",
        "import bisect\n",
        "import functools\n",
        "\n",
        "from torch.utils.data.dataset import Dataset, IterableDataset\n",
        "from typing import (\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Generic,\n",
        "    Iterable,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    Tuple,\n",
        "    TypeVar,\n",
        ")\n",
        "T_co = TypeVar('T_co', covariant=True)\n",
        "\n",
        "class CombinedDataset(Dataset[T_co]):\n",
        "  # modified version of torch.utils.data.dataset.ConcatDataset\n",
        "    r\"\"\"Dataset as a combination of multiple datasets.\n",
        "\n",
        "    This class is a modified version of torch.utils.data.dataset.ConcatDataset,\n",
        "    which is useful to assemble different existing datasets.\n",
        "\n",
        "    Args:\n",
        "        datasets (sequence): List of datasets to be concatenated\n",
        "    \"\"\"\n",
        "    datasets: List[Dataset[T_co]]\n",
        "    cumulative_sizes: List[int]\n",
        "\n",
        "    @staticmethod\n",
        "    def cumsum(sequence):\n",
        "        r, s = [], 0\n",
        "        for e in sequence:\n",
        "            l = len(e)\n",
        "            r.append(l + s)\n",
        "            s += l\n",
        "        return r\n",
        "    \n",
        "    @staticmethod\n",
        "    def images_labels_classes(sequence):\n",
        "        label = 0\n",
        "        #images = []\n",
        "        classes = []\n",
        "        for ds in sequence:\n",
        "          label += len(ds.classes)\n",
        "          #images.extend(ds.imgs)\n",
        "          classes.append(ds.classes)\n",
        "        #return images, list(range(label)), classes\n",
        "        return list(range(label)), classes\n",
        "    \n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def list_labels_for_all_images(datasets, classes):\n",
        "      labels = []\n",
        "      idx = 0\n",
        "      offset = 0\n",
        "      for ds_idx in range(len(datasets)):\n",
        "        l = [instance[1]+offset for instance in datasets[ds_idx].imgs]\n",
        "        labels.extend(l)\n",
        "        offset += len(classes[idx])\n",
        "        idx += 1\n",
        "      return labels\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def mapped_labels(labels, classes):\n",
        "        map = {}\n",
        "        cidx = 0\n",
        "        max_label_idx = len(labels)\n",
        "        for idx in range(len(classes)):\n",
        "          for idc in range(0,len(classes[idx])):\n",
        "              if cidx >= max_label_idx:\n",
        "                  return map\n",
        "              map.update({labels[cidx] : (classes[idx][idc], idc)})\n",
        "              cidx += 1\n",
        "              \n",
        "        return map\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def mapped_items_per_label(datasets, map):\n",
        "        items_per_label = {}\n",
        "        max_idx = len(map.keys())\n",
        "        idx = 0\n",
        "        offset = 0\n",
        "        offset2 = 0\n",
        "        \n",
        "        for ds in datasets:\n",
        "          labels = [instance[1] for instance in ds.imgs]\n",
        "          for i in range(idx, idx+len(ds.classes)):\n",
        "            items_per_label.update({i : [j+offset2 for j in range(len(labels)) if labels[j] == map.get(i)[1]]})\n",
        "            offset = len(ds.classes)\n",
        "          offset2 += len(labels)\n",
        "          idx += offset\n",
        "          #offset2 -= 1\n",
        "        return items_per_label          \n",
        "        \n",
        "\n",
        "    def __init__(self, datasets: Iterable[Dataset]) -> None:\n",
        "        super(CombinedDataset, self).__init__()\n",
        "        self.datasets = list(datasets)\n",
        "        assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]\n",
        "        for d in self.datasets:\n",
        "            assert not isinstance(d, IterableDataset), \"CobinedDataset does not support IterableDataset\"\n",
        "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
        "        \n",
        "        #self.imgs, self.possible_labels, self.grouped_classes = self.images_labels_classes(self.datasets)\n",
        "        self.possible_labels, self.grouped_classes = self.images_labels_classes(self.datasets)\n",
        "        self.classes = [item for sublist in self.grouped_classes for item in sublist]\n",
        "        self.mapped_labels = self.mapped_labels(self.possible_labels, self.grouped_classes)\n",
        "        self.items_per_label = self.mapped_items_per_label(self.datasets, self.mapped_labels)\n",
        "        self.labels = self.list_labels_for_all_images(self.datasets, self.grouped_classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.cumulative_sizes[-1]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < 0:\n",
        "            if -idx > len(self):\n",
        "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
        "            idx = len(self) + idx\n",
        "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
        "        if dataset_idx == 0:\n",
        "            sample_idx = idx\n",
        "        else:\n",
        "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
        "        return (self.datasets[dataset_idx].imgs[sample_idx][0], self.labels[idx])\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "qXKGURSiPzZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ],
      "metadata": {
        "id": "8u1PszLtmByV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title def create_subfilesystem  { form-width: \"15%\", display-mode: \"form\" }\n",
        "def create_subfilesystem(base_path, config):\n",
        "  path_dict = {}\n",
        "  conf_dict = config.pop(\"params\")\n",
        "  for key in config.keys():\n",
        "    p1 = os.path.join(base_path, key)\n",
        "    os.mkdir(p1)\n",
        "    path_dict.update({key:{}})\n",
        "    for subcatkey in config[key].keys():\n",
        "      p2 = os.path.join(p1, subcatkey)\n",
        "      os.mkdir(p2)\n",
        "      path_dict[key].update({subcatkey:[]})\n",
        "      for p_dict in config[key][subcatkey]:\n",
        "        p3 = os.path.join(p2, str(p_dict[subcatkey]))\n",
        "        os.mkdir(p3)\n",
        "        path_dict[key][subcatkey].append(p3)\n",
        "        p4 = os.path.join(p3, \"checkpoints\")\n",
        "        p5 = os.path.join(p3, \"figures\")\n",
        "        os.mkdir(p4)\n",
        "        os.mkdir(p5)\n",
        "  return path_dict, conf_dict"
      ],
      "metadata": {
        "id": "L6M49Cdg1IFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title functions to save to *.csv { form-width: \"15%\", display-mode: \"form\" }\n",
        "def save_performance_as_csv(save_to_path, epochs, train_losses, train_accs, test_losses, test_accs):\n",
        "  with open(save_to_path, 'w', newline='') as csvfile:\n",
        "      header_key = ['epoch','train_loss','train_acc','test_loss', 'test_acc']\n",
        "      \n",
        "      new_val = csv.DictWriter(csvfile, fieldnames=header_key)\n",
        "      new_val.writeheader()\n",
        "\n",
        "      for idx in range(len(epochs)):\n",
        "        new_val.writerow({'epoch': epochs[idx], 'train_loss': train_losses[idx], 'train_acc' : train_accs[idx] , 'test_loss' : test_losses[idx] , 'test_acc': test_accs[idx]})\n",
        "  print(\"Perfomance over time was saved at \"+save_to_path+\"\\n\")\n",
        "\n",
        "  \n",
        "def save_report_as_csv(save_to_path, report_dict):\n",
        "  with open(save_to_path, 'w', newline='') as csvfile:\n",
        "      header_key = ['label','precision','recall','f1-score', 'support']\n",
        "      sec_key = ['accuracy']\n",
        "      new_val = csv.DictWriter(csvfile, fieldnames=header_key)\n",
        "      sec_val = csv.DictWriter(csvfile, fieldnames=sec_key)\n",
        "      new_val.writeheader()\n",
        "      for key in report_dict.keys():\n",
        "          if key != 'accuracy':\n",
        "            new_val.writerow({'label': key, 'precision': report_dict[key]['precision'], 'recall' : report_dict[key]['recall'],'f1-score' : report_dict[key]['f1-score'], 'support' : report_dict[key]['support']})\n",
        "      sec_val.writeheader()\n",
        "      sec_val.writerow({'accuracy' : report_dict['accuracy']})\n",
        "  print(\"Classification report was saved at \"+save_to_path+\"\\n\")\n",
        "\n",
        "def save_track_record_as_csv(save_to_path, t):\n",
        "  with open(save_to_path, 'w', newline='') as csvfile:\n",
        "    config_key = ['n_way', 'n_shot', 'n_query', 'n_evaluation_tasks', 'image_size', 'batch_size', 'n_validation_tasks', 'pretrained_net']\n",
        "    acc_stats_key = ['epochs', 'accuracy']\n",
        "    loss_stats_key = ['epochs', 'loss']\n",
        "    w1 = csv.DictWriter(csvfile, fieldnames=config_key)\n",
        "    w2 = csv.DictWriter(csvfile, fieldnames=acc_stats_key)\n",
        "    w3 = csv.DictWriter(csvfile, fieldnames=loss_stats_key)\n",
        "    w1.writeheader()\n",
        "    w1.writerow({'n_way': t.n_way, 'n_shot': t.n_shot, 'n_query': t.n_query, 'n_evaluation_tasks': t.n_evaluation_tasks, 'image_size': t.image_size, 'batch_size': t.batch_size, 'n_validation_tasks': t.n_validation_tasks, 'pretrained_net': t.backbone})\n",
        "    w2.writeheader()\n",
        "    a = t.acc_logging\n",
        "    for i in a.keys():\n",
        "      w2.writerow({'epochs': i, 'accuracy': a[i]})\n",
        "    l = t.loss_list\n",
        "    w3.writeheader()\n",
        "    for i in range(len(l)):\n",
        "       w3.writerow({'epochs': i, 'loss': l[i]})"
      ],
      "metadata": {
        "id": "OIvzZeEYxTNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # def fetch_model  { form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown This function is used to load model checkpoints and save information about the loaded checkpoint in the StatsTracker object tracker.\n",
        "def fetch_model(load_path_checkpoint):\n",
        "  global tracker\n",
        "  global convolutional_network\n",
        "  model = PrototypicalNetworks(convolutional_network).to(device)\n",
        "  checkpoint = torch.load(load_path_checkpoint)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  tracker.loss_list = checkpoint['loss']\n",
        "  print(checkpoint['loss'])\n",
        "  tracker.epochs = checkpoint['epoch']\n",
        "  \n",
        "  if VERBOSE > 0: print(\"Fetched model {}\\n\".format(load_path_checkpoint))\n",
        "  return model, optimizer, checkpoint['loss'], checkpoint['epoch']"
      ],
      "metadata": {
        "id": "ltFfs2LVFbv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # def find_best_model { form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown This function builds a path to the model with the smallest loss saved in the StatsTracker object's attribute loss_list.\n",
        "def find_best_model():\n",
        "  global tracker\n",
        "  index_min = np.argmin(tracker.loss_list)\n",
        "  load_path_checkpoint = os.path.join(cwd, \"checkpoints\", \"model-epoch_{:05}-loss_{:0.3f}-N_way_{}-N_shot_{}-N_query_{}.pt\".format(index_min, tracker.loss_list[index_min], tracker.n_way, tracker.n_shot, tracker.n_query))\n",
        "  return load_path_checkpoint"
      ],
      "metadata": {
        "id": "BSJKCWSc6csp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #functions to evaluate the model { form-width: \"15%\", display-mode: \"form\" }\n",
        "def evaluate_on_one_task_properly(\n",
        "    support_images: torch.Tensor,\n",
        "    support_labels: torch.Tensor,\n",
        "    query_images: torch.Tensor,\n",
        "    query_labels: torch.Tensor,\n",
        ") -> [int, int]:\n",
        "    \"\"\"\n",
        "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        torch.max(\n",
        "            model(support_images.to(device), support_labels.to(device), query_images.to(device))\n",
        "            .detach()\n",
        "            .data,\n",
        "            1,\n",
        "        )[1]\n",
        "        == query_labels.to(device)\n",
        "    ).sum().item(), len(query_labels), list(zip(support_labels, query_labels))\n",
        "\n",
        "\n",
        "def evaluate_properly(data_loader: DataLoader):\n",
        "    # We'll count everything and compute the ratio at the end\n",
        "    global tracker\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "    total_classifications = []\n",
        "    \n",
        "    \n",
        "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
        "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            class_ids,\n",
        "        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "            \n",
        "            correct, total, classifications = evaluate_on_one_task_properly(\n",
        "                support_images, support_labels, query_images, query_labels\n",
        "            )\n",
        "            _y_true_, _y_pred_ = zip(*classifications)\n",
        "            \n",
        "            y_true_ = [class_ids[i.item()] for i in list(_y_true_)]\n",
        "            y_pred_ = [class_ids[i.item()] for i in list(_y_pred_)]\n",
        "            total_predictions += total\n",
        "            correct_predictions += correct\n",
        "            total_classifications.extend(zip(y_true_, y_pred_))\n",
        "    y_true_, y_pred_ = zip(*total_classifications)\n",
        "\n",
        "    #y_true = [example_class_ids[i] for i in list(y_true_)]\n",
        "    #y_pred = [example_class_ids[i] for i in list(y_pred_)]\n",
        "    y_true = list(y_true_)\n",
        "    y_pred = list(y_pred_)\n",
        "    accuracy = 100 * correct_predictions/total_predictions\n",
        "    \n",
        "    tracker.acc_logging = accuracy\n",
        "    #if verbose == verbose_levels[2]:\n",
        "    if VERBOSE == 2:\n",
        "        print(\"Ground Truth / Predicted\")\n",
        "        for i in range(total_predictions):\n",
        "            \n",
        "            print(\n",
        "                #f\"{total_classifications[i][0]}/{total_classifications[i][1]}\"\n",
        "                f\"{faps_data_test.classes[y_true[i]]} / {faps_data_test.classes[y_pred[i]]}\"\n",
        "            )\n",
        "        print(\"\\n\\n\")\n",
        "    print(\n",
        "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {accuracy:.2f}%\"\n",
        "    )\n",
        "    return y_true, y_pred, accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "YxEOPmwnkFaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # def get_metrics { form-width: \"15%\", display-mode: \"form\" }\n",
        "def get_metrics(data_loader: DataLoader):\n",
        "  global tracker\n",
        "  labels, predictions, acc = evaluate_properly(data_loader)\n",
        "  \n",
        "  if VERBOSE > 0:\n",
        "      fig, ax = plt.subplots(figsize=(20,20))\n",
        "      disp = ConfusionMatrixDisplay.from_predictions(y_true=labels, y_pred=predictions, labels=data_loader.dataset.possible_labels, sample_weight=None, normalize='true', display_labels=data_loader.dataset.classes, include_values=True, xticks_rotation='vertical', values_format=None, cmap=faps_colours, ax=ax, colorbar=True)\n",
        "      save_to_path = \"figures\"\n",
        "      confusion_matrix_filename = \"confusion-matrix_n-way_{}_n-shot_{}_n-query_{}_training-epochs_{}_validation-tasks_{}.svg\".format(tracker.n_way, tracker.n_shot, tracker.n_query, tracker.epochs, tracker.n_validation_tasks)\n",
        "      plt.savefig(os.path.join(cwd, save_to_path, confusion_matrix_filename))\n",
        "  classification_report_filename =  \"classification-report_n-way_{}_n-shot_{}_n-query_{}_training-epochs_{}_validation-tasks_{}.svg\".format(tracker.n_way, tracker.n_shot, tracker.n_query, tracker.epochs, tracker.n_validation_tasks)\n",
        "  #save_report_as_csv(os.path.join(drive_path, save_to_path, classification_report_filename + '.csv'), classification_report(labels, predictions,labels=data_loader.dataset.possible_labels,target_names=data_loader.dataset.classes, output_dict=True, zero_division=0))\n",
        "\n",
        "  if VERBOSE > 0:\n",
        "      print(\"\\n\\n\")\n",
        "      print(classification_report(labels, predictions,labels=data_loader.dataset.possible_labels,target_names=data_loader.dataset.classes, output_dict=False, zero_division=0))\n",
        "  if tracker.epochs == -1:\n",
        "    tracker.base_performance = acc\n",
        "  else:\n",
        "    print(\"\\nModel trained for {} epochs. Accuracy has changed from baseline {:.2f}% to  {:.2f}%.\\nThis means improvement by {:.3f}%\\n\".format(tracker.epochs, tracker.base_performance, acc, acc - tracker.base_performance))\n"
      ],
      "metadata": {
        "id": "DZy769iPxDeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0B1xX1Cb7Krv"
      },
      "source": [
        "#@title # def fit { form-width: \"15%\", display-mode: \"form\" }\n",
        "def prepare_training():\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  return criterion, optimizer\n",
        "\n",
        "def fit(\n",
        "    support_images: torch.Tensor,\n",
        "    support_labels: torch.Tensor,\n",
        "    query_images: torch.Tensor,\n",
        "    query_labels: torch.Tensor,\n",
        ") -> float:\n",
        "    optimizer.zero_grad()\n",
        "    classification_scores = model(\n",
        "        support_images.to(device), support_labels.to(device), query_images.to(device)\n",
        "    )\n",
        "\n",
        "    loss = criterion(classification_scores, query_labels.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #def training { form-width: \"15%\", display-mode: \"form\" }\n",
        "\n",
        "def training(model, tracker, train_loader, test_loader):\n",
        "    global criterion\n",
        "    global optimizer\n",
        "    log_update_frequency = 5\n",
        "    save_model_frequency = 150\n",
        "    get_metrics_frequency = 500\n",
        "    _print = (VERBOSE > 0)\n",
        "    all_loss = tracker.loss_list\n",
        "    offset = max(tracker.epochs, 0)\n",
        "    min_loss = 0.02\n",
        "    loss_cutoff = 0.1\n",
        "    \n",
        "\n",
        "\n",
        "    model.train()\n",
        "    with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n",
        "        tqdm_train.n = offset \n",
        "        tqdm_train.last_print_n = offset\n",
        "        tqdm_train.update()\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            _,\n",
        "        ) in tqdm_train:\n",
        "            loss_value = fit(support_images, support_labels, query_images, query_labels)\n",
        "\n",
        "            all_loss.append(loss_value)\n",
        "\n",
        "            if (episode_index) % get_metrics_frequency == 0 and episode_index > 0:\n",
        "                tracker.loss_list = all_loss\n",
        "                get_metrics(test_loader)\n",
        "\n",
        "            if (episode_index) % log_update_frequency == 0 and episode_index > 0:\n",
        "                tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n",
        "                tracker.loss_list = all_loss\n",
        "            \n",
        "            if (episode_index + offset) % save_model_frequency == 0 and loss_value < loss_cutoff or loss_value < min_loss or episode_index == tqdm_train.total-1:\n",
        "                if loss_value < min_loss:\n",
        "                    min_loss = loss_value\n",
        "                    if _print: print(\"\\nnew min_loss: \"+str(min_loss)+\"\\n\")\n",
        "                torch.save({\n",
        "                'epoch': episode_index + offset,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': all_loss,\n",
        "                }, os.path.join(cwd,\"checkpoints\",\"model-epoch_{:05}-loss_{:0.3f}-N_way_{}-N_shot_{}-N_query_{}.pt\".format(episode_index + offset, loss_value, tracker.n_way, tracker.n_shot, tracker.n_query)))\n",
        "                \n",
        "            tracker.loss_list = all_loss\n",
        "    get_metrics(test_loader)"
      ],
      "metadata": {
        "id": "gdK8pORP-kpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title the ACTUAL algorithm { form-width: \"15%\", display-mode: \"code\" }\n",
        "def run_for_one_param_setting(save_to_path, param_dict):\n",
        "\n",
        "  # read parameters from dict\n",
        "  N_WAY = param_dict[\"n_way\"]\n",
        "  N_SHOT = param_dict[\"n_shot\"]\n",
        "  N_QUERY = param_dict[\"n_query\"]\n",
        "  N_EVALUATION_TASKS = param_dict[\"n_evaluation_tasks\"]\n",
        "  IMAGE_SIZE = param_dict[\"image_size\"]\n",
        "  BATCH_SIZE = param_dict[\"batch_size\"]\n",
        "  N_TRAINING_EPISODES = param_dict[\"n_training_episodes\"]\n",
        "  #N_TRAINING_EPISODES = 2000\n",
        "  N_VALIDATION_TASKS = param_dict[\"n_validation_tasks\"]\n",
        "  PRETRAINED_NET = param_dict[\"pretrained_net\"]\n",
        "  \n",
        "  verbose_levels =  [\"minimal output\", \"moderate output\", \"verbose\"]\n",
        "  global VERBOSE\n",
        "  VERBOSE = verbose_levels.index(param_dict[\"verbose\"])\n",
        "  global cwd\n",
        "  cwd = save_to_path\n",
        "\n",
        "  # create tracker object\n",
        "  global tracker\n",
        "  tracker = StatsTracker(N_WAY, N_SHOT, N_QUERY, N_EVALUATION_TASKS, IMAGE_SIZE, BATCH_SIZE, N_TRAINING_EPISODES, N_VALIDATION_TASKS, PRETRAINED_NET)\n",
        "\n",
        "  # get pretrained backbone\n",
        "  nets = [\"resnet18\", \"alexnet\", \"squeezenet\", \"googlenet\", \"densenet161\"]\n",
        "  global convolutional_network\n",
        "  idx = nets.index(PRETRAINED_NET)\n",
        "  if idx == 0:\n",
        "    convolutional_network = resnet18(pretrained=True)\n",
        "  elif idx == 1:\n",
        "    convolutional_network = alexnet(pretrained=True)\n",
        "  elif idx == 2:\n",
        "    convolutional_network = squeezenet1_0(pretrained=True)\n",
        "  elif idx == 3:\n",
        "    convolutional_network = googlenet(pretrained=True)\n",
        "  elif idx == 4:\n",
        "    convolutional_network = models.densenet161(pretrained=True)\n",
        "  convolutional_network.fc = nn.Flatten() \n",
        "\n",
        "  # create model with backbone\n",
        "  global model\n",
        "  model = PrototypicalNetworks(convolutional_network).to(device)\n",
        "\n",
        "  global criterion\n",
        "  global optimizer\n",
        "  criterion, optimizer = prepare_training()\n",
        "\n",
        "  # create DataLoader object for testing\n",
        "  test_sampler=ModTaskSampler(faps_data_test, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS)\n",
        "  test_loader = DataLoader(\n",
        "      faps_data_test,\n",
        "      batch_sampler=test_sampler,\n",
        "      num_workers=2,\n",
        "      pin_memory=True,\n",
        "      collate_fn=test_sampler.episodic_collate_fn,\n",
        "  )\n",
        "\n",
        "  # create DataLoader object for training\n",
        "  train_sampler=ModTaskSampler(faps_data_train, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES)\n",
        "  train_loader = DataLoader(\n",
        "      faps_data_train,\n",
        "      batch_sampler=train_sampler,\n",
        "      num_workers=2,\n",
        "      pin_memory=True,\n",
        "      collate_fn=train_sampler.episodic_collate_fn,\n",
        "  )\n",
        "\n",
        "  # visualize one task\n",
        "  if VERBOSE > 0:\n",
        "    (\n",
        "        example_support_images,\n",
        "        example_support_labels,\n",
        "        example_query_images,\n",
        "        example_query_labels,\n",
        "        example_class_ids\n",
        "    ) = next(iter(test_loader))\n",
        "    print(\"This task contains the following {} classes {}\\n\".format(N_WAY, example_class_ids))\n",
        "    plot_images(example_support_images, \"support images\", images_per_row=N_SHOT)\n",
        "    plot_images(example_query_images, \"query images\", images_per_row=N_QUERY)\n",
        "\n",
        "  # establish baseline\n",
        "  get_metrics(test_loader)\n",
        "\n",
        "  # train the model and save results\n",
        "  training(model, tracker, train_loader, test_loader)\n",
        "\n",
        "  # fetch the best model again for evaluation, if loss < 0.1, otherwise it wasn't saved\n",
        "  if min(tracker.loss_list) < 0.02:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    index_min = np.argmin(tracker.loss_list)\n",
        "    load_path_checkpoint = find_best_model()\n",
        "    model, optimizer, all_loss, epochs = fetch_model(load_path_checkpoint)\n",
        "    get_metrics(test_loader)\n",
        "\n",
        "  # save info in tracker object\n",
        "  record_filename = \"record_net_{}_n-way_{}_n-shot_{}_n-query_{}_training-epochs_{}_validation-tasks_{}.csv\".format(PRETRAINED_NET, N_WAY, N_SHOT, N_QUERY, tracker.epochs, N_VALIDATION_TASKS)\n",
        "  s_path = os.path.join(cwd, \"figures\", record_filename)\n",
        "  save_track_record_as_csv(s_path, tracker)\n",
        "\n"
      ],
      "metadata": {
        "id": "DORsF1FG8_7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define constants"
      ],
      "metadata": {
        "id": "DWftIaSUmQjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title colourscheme faps_colours { form-width: \"15%\", display-mode: \"form\" }\n",
        "faps_green=(151/255, 193/255, 57/255)\n",
        "faps_dark_green=(93/255,119/255,35/255)\n",
        "faps_light_green=(205/255, 226/255, 158/255)\n",
        "faps_light_yellow=(1, 234/255, 147/255)\n",
        "faps_yellow=(1, 204/255, 0)\n",
        "faps_dark_yellow=(200/255, 162/255, 0)\n",
        "faps_colours = ListedColormap([\"white\", faps_light_yellow, faps_yellow, faps_light_green, faps_green, faps_dark_green])"
      ],
      "metadata": {
        "id": "j1LUQmgh6zra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration: Manual input required"
      ],
      "metadata": {
        "id": "WKwMFW2Mnf_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive { form-width: \"15%\", display-mode: \"form\" }\n",
        "#@markdown This block requires you to go through the login process of your Google Account to access Google Drive where your dataset should be stored.\n",
        "from google.colab import drive\n",
        "base_path = '/content/data'\n",
        "drive.mount(base_path)\n",
        "wd = os.path.join(base_path, \"MyDrive\")"
      ],
      "metadata": {
        "id": "g9vZGh5SgFmQ",
        "outputId": "dfa15bd6-e871-438e-84d5-23f9e8ddb4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title parameter configuration\n",
        "#@markdown Select this option and provide a config to skip the manual parameter entry of the next cell \n",
        "set_parameters_from_config_file = True #@param {type:\"boolean\"}\n",
        "#@markdown Please provide a path relative to the \"root\" level of your Google Drive account including the filename and its filename extension\n",
        "rel_config_path = \"peds3_conf.json\" #@param {type:\"string\"}\n",
        "\n",
        "if set_parameters_from_config_file:\n",
        "  if rel_config_path == \"\":\n",
        "    print(\"Please provide rel_config_path or disable the set_parameters_from_config_file option!\")\n",
        "  config_path = os.path.join(wd, rel_config_path)\n",
        "  with open(config_path) as json_file:\n",
        "    config_json = json.load(json_file)\n",
        "\n",
        "  p = os.path.join(wd, \"PEDS3/PrototypicalNetwork\")\n",
        "  print(config_json.keys())\n",
        "  path_dict, conf_dict = create_subfilesystem(p, config_json)\n",
        "  print(conf_dict.keys())\n",
        "  data_path = conf_dict[\"data_path\"]\n",
        "  drive_path = os.path.join(base_path, \"MyDrive\", data_path)\n",
        "\n",
        "  preferred_device = conf_dict[\"preferred_device\"]\n",
        "  if preferred_device == \"GPU\" and torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Could not connect to GPU, connected to CPU instead!')\n",
        "      device = 'cpu'\n",
        "    else:\n",
        "      print(\"Connected to the following GPU:\\n\")\n",
        "      print(gpu_info)\n",
        "  \n",
        "  image_size = conf_dict[\"image_size\"]\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9P3hjFDjuZQ",
        "outputId": "74e606cf-8562-4987-86e9-0e27164fefa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['params', 'resnet', 'alexnet', 'squeezenet', 'googlenet'])\n",
            "dict_keys(['data_path', 'preferred_device', 'image_size'])\n",
            "Connected to the following GPU:\n",
            "\n",
            "Sat Jan  8 15:33:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title configuration pt 2: manually { form-width: \"15%\", display-mode: \"form\" }\n",
        "\n",
        "if not set_parameters_from_config_file:\n",
        "  #@markdown #### Number of images per class in the support set\n",
        "  n_shot = 4 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "  #@markdown #### Number of classes in a task\n",
        "  n_way = 6 #@param {type:\"slider\", min:1, max:15, step:1}\n",
        "  #@markdown #### Number of images per class in the query set\n",
        "  n_query = 1 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "  n_evaluation_tasks = 100 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "  image_size = 256 #@param {type:\"slider\", min:16, max:512, step:128}\n",
        "  batch_size = 8 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "  n_training_episodes = 5000 #@param {type:\"slider\", min:100, max:7500, step:100}\n",
        "  n_validation_tasks = 50 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
        "  #@markdown ---\n",
        "  #@markdown ### Choose if you want to use GPU or CPU:\n",
        "  preferred_device = \"GPU\" #@param [\"CPU\", \"GPU\"]\n",
        "  #@markdown ---\n",
        "  #@markdown ### Enter a Google Drive path to your dataset:\n",
        "  data_path = \"modified_data_set\" #@param {type:\"string\"}\n",
        "  #@markdown ---\n",
        "\n",
        "  if preferred_device == \"GPU\" and torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "    if gpu_info.find('failed') >= 0:\n",
        "      print('Could not connect to GPU, connected to CPU instead!')\n",
        "      device = 'cpu'\n",
        "    else:\n",
        "      print(\"Connected to the following GPU:\\n\")\n",
        "      print(gpu_info)\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "    print(\"Connected to CPU, this might be slow. Consider connecting to GPU and executing this code cell again!\\n\")\n",
        "\n",
        "  #@markdown ### Choose a pretrained model to start with:\n",
        "  pretrained_net = \"resnet18\" #@param [\"resnet18\", \"alexnet\", \"squeezenet\", \"googlenet\", \"densenet161\"]\n",
        "\n",
        "  #@markdown ---\n",
        "  #@markdown ### How much output do you want to generate:\n",
        "  verbose = \"verbose\" #@param [\"minimal output\", \"moderate output\", \"verbose\"]\n",
        "  verbose_levels = [\"minimal output\", \"moderate output\", \"verbose\"]\n",
        "  \n",
        "  N_WAY = n_way\n",
        "  N_SHOT = n_shot\n",
        "  N_QUERY = n_query\n",
        "  N_EVALUATION_TASKS = n_evaluation_tasks\n",
        "  N_TRAINING_EPISODES = n_training_episodes\n",
        "  N_VALIDATION_TASKS = n_validation_tasks\n",
        "  VERBOSE = verbose_levels.index(verbose)\n",
        "  drive_path = os.path.join(base_path, \"MyDrive\", data_path)\n"
      ],
      "metadata": {
        "id": "UyCwYDbpR7Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Code"
      ],
      "metadata": {
        "id": "2yJHIvwHnwKH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7s0Hiy4F_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "7190ee4f-b648-41fb-8d9a-80dd139131bc"
      },
      "source": [
        "#@title Create datasets { form-width: \"15%\", display-mode: \"form\" }\n",
        "\n",
        "train_data_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.RandomResizedCrop(image_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        ")\n",
        "\n",
        "test_data_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "            transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        ")\n",
        "\n",
        "#create train dataset from all folders in the specified path\n",
        "folder_list = sorted(os.listdir(os.path.join(drive_path, \"train\")))\n",
        "dataset_list = []\n",
        "for folder in folder_list:\n",
        "  ds = datasets.ImageFolder(root=os.path.join(drive_path, \"train\", folder), transform=train_data_transform)\n",
        "  dataset_list.append(ds)\n",
        "\n",
        "faps_data_train = CombinedDataset(dataset_list)\n",
        "\n",
        "\n",
        "\n",
        "#create test dataset from all folders in the specified path\n",
        "folder_list = sorted(os.listdir(os.path.join(drive_path, \"test\")))\n",
        "dataset_test_list = []\n",
        "for folder in folder_list:\n",
        "  ds = datasets.ImageFolder(root=os.path.join(drive_path, \"test\", folder), transform=test_data_transform)\n",
        "  dataset_test_list.append(ds)\n",
        "\n",
        "faps_data_test = CombinedDataset(dataset_test_list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-08686d5d4c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#create train dataset from all folders in the specified path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mfolder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mdataset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/MyDrive/PEDS3/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN manually  { form-width: \"15%\", display-mode: \"form\" }\n",
        "if not set_parameters_from_config_file:\n",
        "  param_dict = {  \n",
        "      \"n_way\": N_WAY,\n",
        "      \"n_shot\": N_SHOT,\n",
        "      \"n_query\": N_QUERY,\n",
        "      \"n_evaluation_tasks\": N_EVALUATION_TASKS,\n",
        "      \"image_size\": image_size,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"n_validation_tasks\": N_VALIDATION_TASKS,\n",
        "      \"pretrained_net\": pretrained_net,\n",
        "      \"verbose\": VERBOSE\n",
        "  }\n",
        "  foldername = \"{}-n_way_{}-n_shot_{}-n_query-{}\".format(pretrained_net, N_WAY, N_SHOT, N_QUERY)\n",
        "  p1 = os.path.join(cwd, foldername)\n",
        "  os.mkdir(p1)\n",
        "  p2 = os.path.join(p1, \"figures\")\n",
        "  p3 = os.path.join(p1, \"checkpoints\")\n",
        "  os.mkdir(p2)\n",
        "  os.mkdir(p3)\n",
        "  run_for_one_param_setting(p1, param_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "y7Tf7K6OyW_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RUN automatically  { form-width: \"15%\", display-mode: \"form\" }\n",
        "for key in config_json.keys():\n",
        "    \n",
        "    for subcatkey in config_json[key].keys():\n",
        "        \n",
        "        for param_dict_idx in range(len(config_json[key][subcatkey])):\n",
        "            run_for_one_param_setting(path_dict[key][subcatkey][param_dict_idx], config_json[key][subcatkey][param_dict_idx])\n"
      ],
      "metadata": {
        "id": "Ra1IpSVMOJuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources"
      ],
      "metadata": {
        "id": "HexvGfNtzwfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code skeleton taken from [here](https://github.com/sicara/easy-few-shot-learning/blob/master/notebooks/my_first_few_shot_classifier.ipynb)\n",
        "\n",
        "The class ModTaskSampler is a modified version of [this](https://github.com/sicara/easy-few-shot-learning/blob/master/easyfsl/data_tools/task_sampler.py)\n",
        "\n",
        "The class CombinedDataset is a modified version of [this](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html)\n"
      ],
      "metadata": {
        "id": "To4Gle1T-scr"
      }
    }
  ]
}